import os
import joblib
import numpy as np
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel, TFRobertaModel
import logging
import warnings
import google.generativeai as genai
from config import API_KEY_GEMINI 
import pandas as pd
import gradio as gr

CSV_FILE_PATH = './chuong_trinh_dao_tao.csv'
MODEL_NAME = 'vinai/phobert-base'
MAX_LENGTH = 128
BASE_SAVE_DIR = './saved_model_output'
WEIGHTS_FILE_NAME = 'my_multitask_model_weights.weights.h5'
ENCODER_NGANH_FILE_NAME = 'encoder_nganh.joblib'
ENCODER_MUC_FILE_NAME = 'encoder_muc.joblib'

MODEL_WEIGHTS_LOAD_PATH = os.path.join(BASE_SAVE_DIR, WEIGHTS_FILE_NAME)
ENCODER_NGANH_PATH = os.path.join(BASE_SAVE_DIR, ENCODER_NGANH_FILE_NAME)
ENCODER_MUC_PATH = os.path.join(BASE_SAVE_DIR, ENCODER_MUC_FILE_NAME)

loaded_model = None
tokenizer_loaded = None
encoder_nganh_loaded = None
encoder_muc_loaded = None
num_classes_nganh_loaded = 0
num_classes_muc_loaded = 0
training_data_dict = {} 

def create_multitask_transformer_model(model_name, num_labels_nganh, num_labels_muc, max_length=MAX_LENGTH):
    base_transformer_layer = TFAutoModel.from_pretrained(model_name, from_pt=True, name="transformer_base_model")
    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name="input_ids")
    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name="attention_mask")
    transformer_outputs = base_transformer_layer(input_ids, attention_mask=attention_mask)
    last_hidden_state = transformer_outputs.last_hidden_state
    cls_token_output = last_hidden_state[:, 0, :]
    dropout_layer = tf.keras.layers.Dropout(0.1)(cls_token_output)
    output_nganh = tf.keras.layers.Dense(num_labels_nganh, activation='softmax', name='output_nganh')(dropout_layer)
    output_muc = tf.keras.layers.Dense(num_labels_muc, activation='softmax', name='output_muc')(dropout_layer)
    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[output_nganh, output_muc])
    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # C·∫ßn compile ƒë·ªÉ load_weights kh√¥ng b√°o l·ªói
    model.compile(optimizer=optimizer,
                  loss={'output_nganh': 'sparse_categorical_crossentropy',
                        'output_muc': 'sparse_categorical_crossentropy'},
                  metrics={'output_nganh': 'accuracy', 'output_muc': 'accuracy'})
    return model

def load_training_data_from_csv():
    global training_data_dict
    try:
        df = pd.read_csv(CSV_FILE_PATH)
        for _, row in df.iterrows():
            key = (str(row['m√£ ng√†nh']).strip(), str(row['m√£ m·ª•c']).strip()) 
            training_data_dict[key] = str(row['n·ªôi dung']).strip() 
        print(f"‚úÖ ƒê√£ t·∫£i {len(training_data_dict)} m·ª•c t·ª´ d·ªØ li·ªáu ƒë√†o t·∫°o CSV.")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file CSV '{CSV_FILE_PATH}': {e}")
        return False

def get_training_content(ma_nganh, ma_muc):
    return training_data_dict.get((ma_nganh, ma_muc), "Kh√¥ng t√¨m th·∫•y n·ªôi dung t∆∞∆°ng ·ª©ng trong d·ªØ li·ªáu ƒë√†o t·∫°o ƒë√£ t·∫£i.")

def load_all_dependencies():
    global loaded_model, tokenizer_loaded, encoder_nganh_loaded, encoder_muc_loaded
    global num_classes_nganh_loaded, num_classes_muc_loaded

    print("--- B·∫ÆT ƒê·∫¶U T·∫¢I C√ÅC TH√ÄNH PH·∫¶N C·∫¶N THI·∫æT ---")
    all_ok = True

    try:
        tokenizer_loaded = AutoTokenizer.from_pretrained(MODEL_NAME)
        print("‚úÖ Tokenizer ƒë√£ t·∫£i.")
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i tokenizer: {e}")
        all_ok = False

    try:
        if os.path.exists(ENCODER_NGANH_PATH) and os.path.exists(ENCODER_MUC_PATH):
            encoder_nganh_loaded = joblib.load(ENCODER_NGANH_PATH)
            encoder_muc_loaded = joblib.load(ENCODER_MUC_PATH)
            num_classes_nganh_loaded = len(encoder_nganh_loaded.classes_)
            num_classes_muc_loaded = len(encoder_muc_loaded.classes_)
            print(f"‚úÖ ƒê√£ t·∫£i encoders: Ng√†nh ({num_classes_nganh_loaded} l·ªõp) | M·ª•c ({num_classes_muc_loaded} l·ªõp)")
        else:
            print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file encoder t·∫°i '{ENCODER_NGANH_PATH}' ho·∫∑c '{ENCODER_MUC_PATH}'.")
            all_ok = False
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i encoders: {e}")
        all_ok = False

    if all_ok and os.path.exists(MODEL_WEIGHTS_LOAD_PATH):
        try:
            if num_classes_nganh_loaded > 0 and num_classes_muc_loaded > 0:
                loaded_model = create_multitask_transformer_model(MODEL_NAME, num_classes_nganh_loaded, num_classes_muc_loaded)
                loaded_model.load_weights(MODEL_WEIGHTS_LOAD_PATH, by_name=True, skip_mismatch=True)
                print("‚úÖ Tr·ªçng s·ªë m√¥ h√¨nh ƒë√£ t·∫£i.")
            else:
                print("‚ùå L·ªói: Thi·∫øu th√¥ng tin s·ªë l·ªõp t·ª´ encoders ƒë·ªÉ t·∫°o l·∫°i m√¥ h√¨nh.")
                all_ok = False
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o m√¥ h√¨nh ho·∫∑c t·∫£i tr·ªçng s·ªë: {e}")
            all_ok = False
    elif all_ok and not os.path.exists(MODEL_WEIGHTS_LOAD_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file tr·ªçng s·ªë m√¥ h√¨nh t·∫°i '{MODEL_WEIGHTS_LOAD_PATH}'.")
        all_ok = False
    
    if all_ok:
        print("‚úÖ T·∫•t c·∫£ c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
    else:
        print("‚ùå M·ªôt s·ªë th√†nh ph·∫ßn kh√¥ng t·∫£i ƒë∆∞·ª£c, vui l√≤ng ki·ªÉm tra l·ªói.")
    return all_ok

def predict_classification(text):
    if not all([loaded_model, tokenizer_loaded, encoder_nganh_loaded, encoder_muc_loaded]):
        return "L·ªói: M√¥ h√¨nh ch∆∞a s·∫µn s√†ng.", "N/A", "N/A"
    try:
        encoding = tokenizer_loaded(text, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')
        inputs = {'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask']}
        pred_nganh_probs, pred_muc_probs = loaded_model.predict(inputs, verbose=0)
        
        pred_label_nganh_encoded = np.argmax(pred_nganh_probs, axis=1)[0]
        pred_label_muc_encoded = np.argmax(pred_muc_probs, axis=1)[0]
        
        ma_nganh_pred = encoder_nganh_loaded.inverse_transform([pred_label_nganh_encoded])[0]
        ma_muc_pred = encoder_muc_loaded.inverse_transform([pred_label_muc_encoded])[0]
        
        confidence_nganh = np.max(pred_nganh_probs)
        confidence_muc = np.max(pred_muc_probs)

        return f"üìå D·ª± ƒëo√°n: Ng√†nh = {ma_nganh_pred} (ƒê·ªô tin c·∫≠y: {confidence_nganh:.2f}), M·ª•c = {ma_muc_pred} (ƒê·ªô tin c·∫≠y: {confidence_muc:.2f})", ma_nganh_pred, ma_muc_pred
    except Exception as e:
        return f"‚ùå L·ªói trong qu√° tr√¨nh d·ª± ƒëo√°n ph√¢n lo·∫°i: {e}", "N/A", "N/A"

def ask_gemini_with_context(user_question, ma_nganh, ma_muc):
    if ma_nganh == "N/A" or ma_muc == "N/A":
        return "Kh√¥ng th·ªÉ l·∫•y n·ªôi dung do l·ªói ph√¢n lo·∫°i tr∆∞·ªõc ƒë√≥."

    noi_dung_tham_khao = get_training_content(ma_nganh, ma_muc)
    
    try:
        genai.configure(api_key=API_KEY_GEMINI)
        generation_config = genai.types.GenerationConfig(temperature=0.7) # ƒêi·ªÅu ch·ªânh nhi·ªát ƒë·ªô n·∫øu c·∫ßn
        model_gemini = genai.GenerativeModel('gemini-2.0-flash', generation_config=generation_config) 
        
        prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh, chuy√™n cung c·∫•p th√¥ng tin v·ªÅ c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ƒë·∫°i h·ªçc.
D·ª±a tr√™n c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√† th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c cung c·∫•p (n·∫øu c√≥), h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, thu h·∫πp trong 1 ƒëo·∫°n t·ª´ 4 ƒë·∫øn 6 c√¢u, r√µ r√†ng v√† h·ªØu √≠ch.

Th√¥ng tin tham kh·∫£o cho c√¢u h·ªèi n√†y:
- Ng√†nh: {ma_nganh}
- M·ª•c: {ma_muc}
- N·ªôi dung chi ti·∫øt:
---
{noi_dung_tham_khao}
---

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: "{user_question}"

H√£y tr·∫£ l·ªùi c√¢u h·ªèi tr√™n. N·∫øu th√¥ng tin tham kh·∫£o kh√¥ng ƒë·ªß ho·∫∑c kh√¥ng li√™n quan tr·ª±c ti·∫øp, h√£y c·ªë g·∫Øng tr·∫£ l·ªùi d·ª±a tr√™n hi·ªÉu bi·∫øt chung v·ªÅ c√°ch c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o th∆∞·ªùng ƒë∆∞·ª£c c·∫•u tr√∫c, nh∆∞ng nh·∫•n m·∫°nh r·∫±ng ƒë√≥ l√† th√¥ng tin chung. N·∫øu kh√¥ng th·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ.
"""
        response = model_gemini.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"‚ùå L·ªói khi g·ªçi Gemini API: {e}"

def chatbot_response(user_question):
    classification_output, nganh_pred, muc_pred = predict_classification(user_question)
    
    gemini_answer = "ƒêang ch·ªù ph√¢n lo·∫°i c√¢u h·ªèi..."
    if nganh_pred != "N/A" and muc_pred != "N/A":
        gemini_answer = ask_gemini_with_context(user_question, nganh_pred, muc_pred)
    elif "L·ªói" in classification_output : 
         gemini_answer = "Kh√¥ng th·ªÉ x·ª≠ l√Ω do l·ªói ph√¢n lo·∫°i."


    return classification_output, gemini_answer

if __name__ == '__main__':
    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    logging.getLogger("transformers").setLevel(logging.ERROR)
    warnings.filterwarnings('ignore', category=UserWarning) 
    warnings.filterwarnings('ignore', category=FutureWarning) 

    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(f"L·ªói c·∫•u h√¨nh GPU: {e}")

    if load_all_dependencies() and load_training_data_from_csv():
        print("üöÄ Kh·ªüi t·∫°o giao di·ªán Gradio...")
        iface = gr.Interface(
            fn=chatbot_response,
            inputs=gr.Textbox(lines=3, placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o..."),
            outputs=[
                gr.Textbox(label="K·∫øt qu·∫£ Ph√¢n lo·∫°i (Ng√†nh/M·ª•c)"),
                gr.Markdown(label="ü§ñ Tr·∫£ l·ªùi t·ª´ Tr·ª£ l√Ω AI")
            ],
            title="üéì Tr·ª£ L√Ω Th√¥ng Tin Tuy·ªÉn Sinh & ƒê√†o T·∫°o üéì",
            description="ƒê·∫∑t c√¢u h·ªèi v·ªÅ c√°c ng√†nh h·ªçc v√† nh·∫≠n c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi AI. M√¥ h√¨nh s·∫Ω c·ªë g·∫Øng ph√¢n lo·∫°i c√¢u h·ªèi c·ªßa b·∫°n v√†o ng√†nh v√† m·ª•c ph√π h·ª£p, sau ƒë√≥ s·ª≠ d·ª•ng th√¥ng tin ƒë√≥ ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi.",
            theme=gr.themes.Soft(),
            allow_flagging='never'
        )
        iface.launch(share=False)
    else:
        print("Kh√¥ng th·ªÉ kh·ªüi ch·∫°y ·ª©ng d·ª•ng do l·ªói t·∫£i c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt.")